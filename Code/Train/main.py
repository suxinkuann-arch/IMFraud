import os
import torch
from transformers import AutoTokenizer

from config.config import TrainingConfig
from data.data_loader import load_dataset, view_data_distribution
from data.preprocess import preprocess_datasets
from model.model_loader import load_model, cleanup_memory
from model.peft_config import build_peft_model
from training.trainer_builder import build_trainer
from utils.helpers import setup_environment, memory_cleanup, get_torch_dtype

import warnings


def main():
    warnings.filterwarnings("ignore")
    # è®¾ç½®é…ç½®
    config = TrainingConfig()
    setup_environment(config)

    # è®¾ç½®åˆ†å¸ƒå¼è®­ç»ƒ
    local_rank = int(os.environ.get("LOCAL_RANK", 0))
    torch.cuda.set_device(local_rank)

    print(f"å¼€å§‹è®­ç»ƒï¼Œä½¿ç”¨è®¾å¤‡: {local_rank}")

    # åŠ è½½tokenizer
    tokenizer = AutoTokenizer.from_pretrained(
        config.MODEL_PATH,
        use_fast=False,
        trust_remote_code=True
    )

    if tokenizer.pad_token is None:
        print("æ£€æµ‹åˆ°tokenizerç¼ºå°‘pad_tokenï¼Œæ­£åœ¨è®¾ç½®...")
        if tokenizer.eos_token is not None:
            tokenizer.pad_token = tokenizer.eos_token
            print(f"è®¾ç½®pad_tokenä¸ºeos_token: {tokenizer.eos_token}")
        else:
            tokenizer.add_special_tokens({'pad_token': '[PAD]'})
            print("æ·»åŠ æ–°çš„pad_token: [PAD]")

    # æŸ¥çœ‹æ•°æ®åˆ†å¸ƒ
    if local_rank == 0:
        view_data_distribution(config.TRAIN_DATA_PATH, show_first=True)

    # åŠ è½½å’Œé¢„å¤„ç†æ•°æ®
    train_ds, eval_ds = load_dataset(config.TRAIN_DATA_PATH, config.EVAL_DATA_PATH)
    train_dataset, eval_dataset = preprocess_datasets(
        train_ds, eval_ds, tokenizer, config.MAX_LENGTH
    )

    if local_rank == 0:
        print(f"è®­ç»ƒé›†å¤§å°: {len(train_dataset)}")
        print(f"éªŒè¯é›†å¤§å°: {len(eval_dataset)}")
        print(f"Pad token: {tokenizer.pad_token}")
        print(f"Pad token ID: {tokenizer.pad_token_id}")

    # æ¸…ç†å†…å­˜
    memory_cleanup()

    # åŠ è½½æ¨¡å‹
    torch_dtype = get_torch_dtype(config.TORCH_DTYPE)
    model = load_model(config.MODEL_PATH, torch_dtype)

    if model.config.pad_token_id is None:
        model.config.pad_token_id = tokenizer.pad_token_id
        print(f"è®¾ç½®æ¨¡å‹config.pad_token_idä¸º: {tokenizer.pad_token_id}")

    if tokenizer.pad_token is not None and tokenizer.pad_token not in tokenizer.get_vocab():
        model.resize_token_embeddings(len(tokenizer))
        print("è°ƒæ•´æ¨¡å‹è¯åµŒå…¥å¤§å°ä»¥åŒ¹é…æ–°çš„pad_token")

    # æ„å»ºLoRAé…ç½®
    lora_config = {
        "target_modules": config.LORA_TARGET_MODULES,
        "r": config.LORA_R,
        "lora_alpha": config.LORA_ALPHA,
        "lora_dropout": config.LORA_DROPOUT
    }

    # æ„å»ºPEFTæ¨¡å‹
    peft_model = build_peft_model(model, lora_config)

    # æ„å»ºè®­ç»ƒå™¨å¹¶å¼€å§‹è®­ç»ƒ
    trainer = build_trainer(
        peft_model,
        tokenizer,
        train_dataset,
        eval_dataset,
        config.OUTPUT_PATH,
        config
    )

    # å¼€å§‹è®­ç»ƒ
    print("å¼€å§‹æ¨¡å‹è®­ç»ƒ...")
    trainer.train()

    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    if local_rank == 0:
        trainer.save_model()
        print(f"è®­ç»ƒå®Œæˆï¼Œæ¨¡å‹å·²ä¿å­˜åˆ°: {config.OUTPUT_PATH}")

    # æœ€ç»ˆå†…å­˜æ¸…ç†
    cleanup_memory()


if __name__ == "__main__":
    main()









#BMBä¸“ç”¨train
# import os
# import torch
# from transformers import AutoTokenizer
#
# from config.config import TrainingConfig
# from data.data_loader import load_dataset, view_data_distribution
# from data.preprocess import preprocess_datasets
# from model.model_loader import load_model, cleanup_memory
# from model.peft_config import build_peft_model
# from training.trainer_builder import build_trainer
# from utils.helpers import setup_environment, memory_cleanup, get_torch_dtype
#
# import warnings
#
# # ğŸ”§ åº•å±‚ä¿®å¤ï¼šMiniCPM4 æ¨¡å‹å…¼å®¹æ€§è¡¥ä¸
# import transformers
# from transformers.cache_utils import DynamicCache
#
# # ä¿å­˜åŸå§‹çš„ _reorder_cache å‡½æ•°
# original_reorder_cache = getattr(transformers, "_reorder_cache", None)
#
#
# def patched_reorder_cache(past_key_values, beam_idx):
#     """ä¿®å¤ MiniCPM4 æ¨¡å‹çš„ç¼“å­˜æ ¼å¼é—®é¢˜"""
#     if isinstance(past_key_values, tuple) and original_reorder_cache is not None:
#         # å°†æ—§æ ¼å¼è½¬æ¢ä¸ºæ–°æ ¼å¼
#         cache = DynamicCache.from_legacy_cache(past_key_values)
#         return cache
#     return past_key_values
#
#
# # æ›¿æ¢ transformers çš„ _reorder_cache å‡½æ•°
# transformers._reorder_cache = patched_reorder_cache
#
#
# # ä¿®å¤æ¨¡å‹çš„ forward æ–¹æ³•
# def patch_model_forward(model):
#     """ä¸º MiniCPM4 æ¨¡å‹æ‰“è¡¥ä¸ï¼Œä¿®å¤ past_key_values æ ¼å¼é—®é¢˜"""
#     if hasattr(model, "forward") and hasattr(model, "config"):
#         original_forward = model.forward
#
#         def new_forward(*args, **kwargs):
#             # æ£€æŸ¥å¹¶è½¬æ¢ past_key_values æ ¼å¼
#             if "past_key_values" in kwargs and isinstance(kwargs["past_key_values"], tuple):
#                 try:
#                     kwargs["past_key_values"] = DynamicCache.from_legacy_cache(kwargs["past_key_values"])
#                 except Exception as e:
#                     print(f"[DEBUG] è½¬æ¢ past_key_values æ—¶å‡ºé”™: {e}")
#                     kwargs["past_key_values"] = None
#
#             # è°ƒç”¨åŸå§‹ forward
#             outputs = original_forward(*args, **kwargs)
#
#             # ç¡®ä¿è¾“å‡ºä¸­çš„ past_key_values ä¹Ÿæ˜¯æ­£ç¡®æ ¼å¼
#             if hasattr(outputs, "past_key_values") and isinstance(outputs.past_key_values, tuple):
#                 outputs.past_key_values = DynamicCache.from_legacy_cache(outputs.past_key_values)
#
#             return outputs
#
#         model.forward = new_forward
#     return model
#
#
# def main():
#     warnings.filterwarnings("ignore")
#     # è®¾ç½®é…ç½®
#     config = TrainingConfig()
#     setup_environment(config)
#
#     # è®¾ç½®åˆ†å¸ƒå¼è®­ç»ƒ
#     local_rank = int(os.environ.get("LOCAL_RANK", 0))
#     torch.cuda.set_device(local_rank)
#
#     print(f"å¼€å§‹è®­ç»ƒï¼Œä½¿ç”¨è®¾å¤‡: {local_rank}")
#
#     # ğŸ”§ å…³é”®ä¿®å¤ï¼šè®¾ç½® use_cache=False ä»¥é¿å…ä¸æ¢¯åº¦æ£€æŸ¥ç‚¹å†²çª
#     config.USE_CACHE = False  # å¦‚æœ config ä¸­æ²¡æœ‰è¿™ä¸ªå±æ€§ï¼Œå¯ä»¥åœ¨æ¨¡å‹åŠ è½½æ—¶è®¾ç½®
#
#     # åŠ è½½tokenizer
#     tokenizer = AutoTokenizer.from_pretrained(
#         config.MODEL_PATH,
#         use_fast=False,
#         trust_remote_code=True
#     )
#
#     # ğŸ”§ å…³é”®ä¿®å¤ï¼šè®¾ç½®padding token
#     if tokenizer.pad_token is None:
#         print("æ£€æµ‹åˆ°tokenizerç¼ºå°‘pad_tokenï¼Œæ­£åœ¨è®¾ç½®...")
#         if tokenizer.eos_token is not None:
#             tokenizer.pad_token = tokenizer.eos_token
#             print(f"è®¾ç½®pad_tokenä¸ºeos_token: {tokenizer.eos_token}")
#         else:
#             tokenizer.add_special_tokens({'pad_token': '[PAD]'})
#             print("æ·»åŠ æ–°çš„pad_token: [PAD]")
#
#     # æŸ¥çœ‹æ•°æ®åˆ†å¸ƒ
#     if local_rank == 0:
#         view_data_distribution(config.TRAIN_DATA_PATH, show_first=True)
#
#     # åŠ è½½å’Œé¢„å¤„ç†æ•°æ®
#     train_ds, eval_ds = load_dataset(config.TRAIN_DATA_PATH, config.EVAL_DATA_PATH)
#     train_dataset, eval_dataset = preprocess_datasets(
#         train_ds, eval_ds, tokenizer, config.MAX_LENGTH
#     )
#
#     if local_rank == 0:
#         print(f"è®­ç»ƒé›†å¤§å°: {len(train_dataset)}")
#         print(f"éªŒè¯é›†å¤§å°: {len(eval_dataset)}")
#         print(f"Pad token: {tokenizer.pad_token}")
#         print(f"Pad token ID: {tokenizer.pad_token_id}")
#
#     # æ¸…ç†å†…å­˜
#     memory_cleanup()
#
#     # åŠ è½½æ¨¡å‹
#     torch_dtype = get_torch_dtype(config.TORCH_DTYPE)
#     model = load_model(config.MODEL_PATH, torch_dtype)
#
#     # ğŸ”§ å…³é”®ä¿®å¤ 1ï¼šåº”ç”¨æ¨¡å‹è¡¥ä¸
#     print("ğŸ”§ åº”ç”¨ MiniCPM4 æ¨¡å‹å…¼å®¹æ€§è¡¥ä¸...")
#     model = patch_model_forward(model)
#
#     # ğŸ”§ å…³é”®ä¿®å¤ 2ï¼šè®¾ç½® use_cache=False
#     if hasattr(model.config, "use_cache"):
#         print("ğŸ”§ è®¾ç½® use_cache=False ä»¥é¿å…ä¸æ¢¯åº¦æ£€æŸ¥ç‚¹å†²çª")
#         model.config.use_cache = False
#
#     # ğŸ”§ å…³é”®ä¿®å¤ 3ï¼šç¡®ä¿æ¨¡å‹é…ç½®ä¸tokenizerä¸€è‡´
#     if model.config.pad_token_id is None:
#         model.config.pad_token_id = tokenizer.pad_token_id
#         print(f"è®¾ç½®æ¨¡å‹config.pad_token_idä¸º: {tokenizer.pad_token_id}")
#
#     # å¦‚æœæ·»åŠ äº†æ–°çš„pad_tokenï¼Œéœ€è¦è°ƒæ•´æ¨¡å‹è¯åµŒå…¥å¤§å°
#     if hasattr(tokenizer, 'get_vocab') and tokenizer.pad_token not in tokenizer.get_vocab():
#         model.resize_token_embeddings(len(tokenizer))
#         print("è°ƒæ•´æ¨¡å‹è¯åµŒå…¥å¤§å°ä»¥åŒ¹é…æ–°çš„pad_token")
#
#     # æ¸…ç†å†…å­˜
#     memory_cleanup()
#
#     # æ„å»ºLoRAé…ç½®
#     lora_config = {
#         "target_modules": config.LORA_TARGET_MODULES,
#         "r": config.LORA_R,
#         "lora_alpha": config.LORA_ALPHA,
#         "lora_dropout": config.LORA_DROPOUT
#     }
#
#     # æ„å»ºPEFTæ¨¡å‹
#     peft_model = build_peft_model(model, lora_config)
#
#     # ğŸ”§ å…³é”®ä¿®å¤ 4ï¼šåœ¨æ„å»º trainer å‰å†æ¬¡åº”ç”¨è¡¥ä¸
#     print("ğŸ”§ å†æ¬¡åº”ç”¨æ¨¡å‹è¡¥ä¸åˆ° PEFT æ¨¡å‹...")
#     if hasattr(peft_model, "base_model"):
#         patch_model_forward(peft_model.base_model)
#     else:
#         patch_model_forward(peft_model)
#
#     # æ„å»ºè®­ç»ƒå™¨
#     trainer = build_trainer(
#         peft_model,
#         tokenizer,
#         train_dataset,
#         eval_dataset,
#         config.OUTPUT_PATH,
#         config
#     )
#
#     # ğŸ”§ å…³é”®ä¿®å¤ 5ï¼šæ·»åŠ åˆ†å¸ƒå¼èµ„æºæ¸…ç†
#     import atexit
#     import torch.distributed as dist
#
#     def cleanup_distributed():
#         """æ¸…ç†åˆ†å¸ƒå¼è®­ç»ƒèµ„æº"""
#         if dist.is_initialized():
#             print("ğŸ”§ æ¸…ç†åˆ†å¸ƒå¼è®­ç»ƒèµ„æº...")
#             try:
#                 dist.barrier()
#                 dist.destroy_process_group()
#                 print("âœ… åˆ†å¸ƒå¼èµ„æºå·²æ¸…ç†")
#             except Exception as e:
#                 print(f"âš ï¸ æ¸…ç†åˆ†å¸ƒå¼èµ„æºæ—¶å‡ºé”™: {e}")
#
#     atexit.register(cleanup_distributed)
#     print("âœ… å·²æ³¨å†Œåˆ†å¸ƒå¼èµ„æºæ¸…ç†å‡½æ•°")
#
#     # å¼€å§‹è®­ç»ƒ
#     print("ğŸš€ å¼€å§‹æ¨¡å‹è®­ç»ƒ...")
#     try:
#         trainer.train()
#     except Exception as e:
#         print(f"âŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
#         # ç¡®ä¿åœ¨å¼‚å¸¸æ—¶ä¹Ÿæ¸…ç†èµ„æº
#         cleanup_distributed()
#         raise
#
#     # ä¿å­˜æœ€ç»ˆæ¨¡å‹
#     if local_rank == 0:
#         try:
#             trainer.save_model()
#             print(f"âœ… è®­ç»ƒå®Œæˆï¼Œæ¨¡å‹å·²ä¿å­˜åˆ°: {config.OUTPUT_PATH}")
#         except Exception as e:
#             print(f"âš ï¸ ä¿å­˜æ¨¡å‹æ—¶å‡ºé”™: {e}")
#
#     # æ‰‹åŠ¨è°ƒç”¨æ¸…ç†
#     cleanup_distributed()
#
#     # æœ€ç»ˆå†…å­˜æ¸…ç†
#     cleanup_memory()
#     print("âœ… è®­ç»ƒè¿‡ç¨‹å®Œå…¨ç»“æŸ")
#
#
# if __name__ == "__main__":
#     main()